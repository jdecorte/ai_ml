{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/jdecorte/ai_ml/blob/main/121-pretrained_word_embeddings_case_spam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpMifYkx85is",
        "outputId": "7bcf8530-07df-433f-b478-895c35043087"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jcor864\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\jcor864\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sk\n",
        "import pandas as pd\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 2020\n",
        "np.random.seed(seed)  \n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Embedding, Conv1D,  MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.constraints import max_norm\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ms0fWnI2PyL2"
      },
      "outputs": [],
      "source": [
        "# helper functions for visualisation\n",
        "# plotting the loss functions used in this notebook\n",
        "# we plot the loss we want to optimise on the left (in this case: accuracy)\n",
        "def plot_history(history):\n",
        "  plt.figure(figsize = (12,4))\n",
        "  plt.subplot(1,2,1)\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(history.epoch, np.array(history.history['accuracy']),'g-',\n",
        "           label='Train accuracy')\n",
        "  plt.plot(history.epoch, np.array(history.history['val_accuracy']),'r-',\n",
        "           label = 'Validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss minimised by model')\n",
        "  plt.plot(history.epoch, np.array(history.history['loss']),'g-',\n",
        "           label='Train loss')\n",
        "  plt.plot(history.epoch, np.array(history.history['val_loss']),'r-',\n",
        "           label = 'Validation loss')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-FrOQlg_Z-D",
        "outputId": "71a67923-173f-47e0-f101-a6135c7167ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label    object\n",
              "text     object\n",
              "dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_dataset = pd.read_csv(\"https://raw.githubusercontent.com/jdecorte/ai_ml/main/datasets/SMSSpamCollection.csv\") \n",
        "df_dataset.columns\n",
        "df_dataset.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ljOBisG4Eju7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Take a look at the data\n",
        "df_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "J8Jl5bZZRIbF",
        "outputId": "4d75789b-51f5-4905-8765-2ea4f2d22358"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      1  Go until jurong point, crazy.. Available only ...\n",
              "1      1                      Ok lar... Joking wif u oni...\n",
              "2      0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      1  U dun say so early hor... U c already then say...\n",
              "4      1  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Changing spam and ham into 0 and 1\n",
        "df_dataset['label'] = np.where(df_dataset['label'] == \"spam\", 0, 1)\n",
        "df_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8jeo1-3RGIFC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5572.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.865937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.340751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             label\n",
              "count  5572.000000\n",
              "mean      0.865937\n",
              "std       0.340751\n",
              "min       0.000000\n",
              "25%       1.000000\n",
              "50%       1.000000\n",
              "75%       1.000000\n",
              "max       1.000000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the general information about the data\n",
        "df_dataset.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import string\n",
        "    \n",
        "def remove_stopwords_en(text):\n",
        "    stop_words_en = set(stopwords.words('english')) \n",
        "    punctuations=\"?:!.,;<>/\\+-\"\n",
        "    # turn the string into a list of words based on separators (blank, comma, etc.)\n",
        "    word_tokens = word_tokenize(text.lower())\n",
        "    # create a list of all words that are neither stopwords nor punctuations\n",
        "    result = [x for x in word_tokens if x not in stop_words_en and x not in punctuations]\n",
        "    \n",
        "    # create a new string of all remaining words\n",
        "    seperator = ' '\n",
        "    return seperator.join(result)\n",
        "\n",
        "df_dataset['text'] = df_dataset['text'].apply(remove_stopwords_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "jpTHHeWgG0ce",
        "outputId": "f0e187bf-7d79-4682-aeca-4646f716d940"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>numberOfWords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>go jurong point crazy .. available bugis n gre...</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ok lar ... joking wif u oni ...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>u dun say early hor ... u c already say ...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>nah n't think goes usf lives around though</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text  numberOfWords\n",
              "0      1  go jurong point crazy .. available bugis n gre...             19\n",
              "1      1                    ok lar ... joking wif u oni ...              8\n",
              "2      0  free entry 2 wkly comp win fa cup final tkts 2...             30\n",
              "3      1        u dun say early hor ... u c already say ...             11\n",
              "4      1         nah n't think goes usf lives around though              8"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# What is the average length of the SMS messages. This will be important when we need to truncate the sequences to a maximum length\n",
        "df_dataset['numberOfWords'] = df_dataset.text.str.split().apply(len)\n",
        "df_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l-4cl8dqHUyU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    5572.000000\n",
              "mean       10.536432\n",
              "std         8.034073\n",
              "min         0.000000\n",
              "25%         5.000000\n",
              "50%         8.000000\n",
              "75%        15.000000\n",
              "max       134.000000\n",
              "Name: numberOfWords, dtype: float64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the general information about the column numberOfWords\n",
        "# Notice that 75% of the messages consists of only 23 words or less.\n",
        "df_dataset['numberOfWords'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK5zHfTOBQs7",
        "outputId": "57cd2aea-6d28-4326-89a0-129484b9454b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (3900, 1)\n",
            "X_test shape: (1672, 1)\n",
            "3900 train samples\n",
            "1672 test samples\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "# Extract a training & validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df_dataset.drop(['label','numberOfWords'],axis=1)\n",
        "y = df_dataset['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "print(type(X_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For Keras we need to convert pandas dataframes to numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsAXPQD9cViO",
        "outputId": "86a329f2-3484-42f4-af63-d44b7c1b8205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (3900, 1)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "X_train = np.asarray(X_train)\n",
        "X_test = np.asarray(X_test)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(type(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YNIcfYeQ8QB",
        "outputId": "f1140ff9-4550-47a4-9333-55dc2ee0c0e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "y_train shape: (3900,)\n",
            "y_test shape: (1672,)\n"
          ]
        }
      ],
      "source": [
        "# look at the label for the first sample\n",
        "print(y_train[0])\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSvPJXT0C0Q8"
      },
      "source": [
        "### Create a vocabulary index\n",
        "\n",
        "Let's use the TextVectorization to index the vocabulary found in the dataset. \n",
        "\n",
        "TextVectorization = \n",
        "- build vocabulary from complete text collection (= all sms's)\n",
        "- assign to each word an index\n",
        "- replace words in dataset by index\n",
        "- limit each text to an maximum length\n",
        "- pad shorter texts with ''\n",
        "\n",
        "Deep Learning systems are often trained on very large datasets that will not fit in RAM. Ingesting a large dataset and preprocessing it efficiently can be tricky to implement with other Deep Learning libraries, but TensorFlow (on which Keras is based) makes it easy thanks to the Data API: you just create a **dataset** object, and tell it where to get the data and how to transform it. TensorFlow takes care of all the implementation details, such as multithreading, queuing, batching, and prefetching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t9vR-3X7Cy7D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\jcor864\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\jcor864\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import TextVectorization\n",
        "\n",
        "\n",
        "# max_tokens = integer parameter that will control the maximum size of the vocabulary. \n",
        "# We will only consider the top 20 000 words\n",
        "# output_sequence_length = 25: we will truncate or pad sequences to be actually 25 tokens long.\n",
        "# This is the reason why we calculated the number of words for each message in a previouse step\n",
        "# From the describe we learn that over 75% of the messages are not longer than 25 tokens\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=25)\n",
        "\n",
        "# Make a Tensorflow Dataset from a numpy array. \n",
        "# A tf.data.Dataset represents a potentially large set of elements.\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "\n",
        "# Call the adapt method to build the vocabulary\n",
        "vectorizer.adapt(text_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQx8k5FECtSU",
        "outputId": "19546e91-db30-4d97-ce8c-8e8446292895"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'u',\n",
              " 'call',\n",
              " 's',\n",
              " '2',\n",
              " 'nt',\n",
              " 'ur',\n",
              " 'm',\n",
              " 'get',\n",
              " 'gt',\n",
              " 'lt',\n",
              " '4',\n",
              " 'free',\n",
              " 'ok',\n",
              " 'go',\n",
              " 'know',\n",
              " 'good',\n",
              " 'got',\n",
              " 'like',\n",
              " 'come',\n",
              " 'll',\n",
              " 'time',\n",
              " 'love',\n",
              " 'day']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can retrieve the computed vocabulary used via vectorizer.get_vocabulary(). \n",
        "# Let's print the top 25 words:\n",
        "vectorizer.get_vocabulary()[:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- index 0 is reserved for the padding token\n",
        "- index 1 is reserved for \"out of vocabulary\" tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrXr5Ry8Dn_j",
        "outputId": "f9d1387c-8a25-45b4-b449-838f4ca37ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[826,   1,   3,   1, 137,  55,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "      dtype=int64)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's vectorize a test sentence, based on the vocabulary we created from the training dataset):\n",
        "output = vectorizer([[\"i will call you next week\"]])\n",
        "print(type(output))\n",
        "output.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"will\" and \"your\" are missing because they have been removed during stopword removal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Iuzq59D-fb"
      },
      "source": [
        "Here's a dictionary mapping words of the vocabulary of the dataset to their indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cYtTe3gQDwEA"
      },
      "outputs": [],
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Eax6cpESR2"
      },
      "source": [
        "As you can see, we obtain the same encoding as above for our test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHO0wHTTENcE",
        "outputId": "ebc75077-2dc4-43b1-bced-0626762e27b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[826, 'N/A', 3, 'N/A', 137, 55]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = [\"i\",\"will\",\"call\", \"you\", \"next\", \"week\"]\n",
        "[word_index.get(w,'N/A') for w in test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymT35BgXEbM7"
      },
      "source": [
        "### Load pre-trained word embeddings\n",
        "Let's download pre-trained GloVe embeddings (a 822M zip file).\n",
        "Rather than training our own word vectors from scratch, we will leverage on GloVe. Its authors have released four text files with word vectors trained on different massive web datasets.\n",
        "\n",
        "The archive contains text-encoded vectors of various sizes: 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n",
        "\n",
        "glove.6B = Wikipedia 2014 + Gigaword 5. It was trained on a corpus of 6 billion tokens and contains a vocabulary of 400 000 tokens.\n",
        "\n",
        "You'll need to run the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AybfgXLHEimc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are not running on Google Colab\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    colab = True\n",
        "    print ('You are running on Google Colab')\n",
        "else:\n",
        "    colab = False\n",
        "    print ('You are not running on Google Colab')\n",
        "\n",
        "if colab:\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    !ls '/content'\n",
        "    !unzip -q glove.6B.zip\n",
        "    # After unzipping the downloaded file we find 4 txt files: glove.6B.50d.txt, glove.6B.100d.txt, glove.6B.200d.txt, glove.6B.300d.txt. \n",
        "    # As their filenames suggests, they have vectors with different dimensions.\n",
        "    !ls '/content'\n",
        "    # move glove.6B.100d.txt to My Drive so it will be available for use in the future\n",
        "    !mv '/content/glove.6B.100d.txt' '/content/gdrive/My Drive/glove.6B.100d.txt'\n",
        "\n",
        "# if your are working local you can download the file from http://nlp.stanford.edu/data/glove.6B.zip and unzip it in your datasets directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is4_imN2E2Z3"
      },
      "source": [
        "If we used glove.6B.50d.txt and we printed  the content of the file on console, we could see that each line contain as first element a word followed by 50 real numbers. For instance these are the first two lines, corresponding to tokens \"the\" and \",\":\n",
        "\n",
        "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
        "\n",
        ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
        "\n",
        "Let's make a dict mapping words (strings) to their NumPy vector representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcp1cFCvFASx",
        "outputId": "11271999-0e84-4499-917b-4da63b609171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# we compute an index mapping words to known embeddings\n",
        "# by parsing the data dump of pre-trained embeddings:\n",
        "\n",
        "if colab: \n",
        "  path_to_glove_file = '/content/gdrive/My Drive/glove.6B.100d.txt'\n",
        "else:\n",
        "  path_to_glove_file = 'datasets/glove.6B.100d.txt'\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "with open(path_to_glove_file, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fo3bId0FUVh"
      },
      "source": [
        "Now, let's prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. It's a simple NumPy matrix where entry at index $i$ is the pre-trained vector for the word of index $i$ in our vectorizer's vocabulary. That means we are mapping words from the vocabulary of our dataset to their corresponding coordinates in the 100 dimensional geometric space from Glove. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrIogMZQFXw9",
        "outputId": "1a137f70-ce23-4322-f8d1-c4751e0cbaef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 5398 words (2076 misses)\n",
            "*** shape of the embedding matrix:***\n",
            "(7474, 100)\n",
            "*** Missed words = words not in word_index ***\n",
            "['', '[UNK]', 'thanx', '£1000', '150ppm', 'aight', '150p', '£2000', '£150', '£100']\n",
            "\n",
            "*** i has which index in word_index? ***\n",
            "826\n",
            "\n",
            "*** the vector of 100 floats representing i ***\n",
            "[-0.046539    0.61966002  0.56647003 -0.46584001 -1.18900001  0.44599\n",
            "  0.066035    0.31909999  0.14679    -0.22119001  0.79238999  0.29905\n",
            "  0.16073     0.025324    0.18678001 -0.31000999 -0.28108001  0.60514998\n",
            " -1.0654      0.52476001  0.064152    1.03579998 -0.40779001 -0.38011\n",
            "  0.30801001  0.59964001 -0.26991001 -0.76034999  0.94221997 -0.46919\n",
            " -0.18278     0.90652001  0.79671001  0.24824999  0.25713     0.6232\n",
            " -0.44768     0.65357     0.76902002 -0.51229    -0.44332999 -0.21867\n",
            "  0.38370001 -1.14830005 -0.94397998 -0.15062     0.30012    -0.57805997\n",
            "  0.20175    -1.65910006 -0.079195    0.026423    0.22051001  0.99713999\n",
            " -0.57538998 -2.72659993  0.31448001  0.70521998  1.43809998  0.99125999\n",
            "  0.13976     1.34739995 -1.1753      0.0039503   1.02980006  0.064637\n",
            "  0.90886998  0.82871997 -0.47003001 -0.10575     0.5916     -0.42210001\n",
            "  0.57331002 -0.54114002  0.10768     0.39783999 -0.048744    0.064596\n",
            " -0.61436999 -0.28600001  0.50669998 -0.49757999 -0.81569999  0.16407999\n",
            " -1.96300006 -0.26693001 -0.37593001 -0.95846999 -0.85839999 -0.71577001\n",
            " -0.32343    -0.43121001  0.41391999  0.28374001 -0.70931     0.15003\n",
            " -0.2154     -0.37616    -0.032502    0.80620003]\n",
            "\n",
            "*** cat has which index in word_index? ***\n",
            "2032\n",
            "\n",
            "*** the vector of 100 floats representing cat ***\n",
            "[ 0.23088001  0.28283     0.6318     -0.59411001 -0.58599001  0.63255\n",
            "  0.24402    -0.14108001  0.060815   -0.78979999 -0.29102001  0.14286999\n",
            "  0.72273999  0.20428     0.1407      0.98756999  0.52533001  0.097456\n",
            "  0.8822      0.51221001  0.40204     0.21168999 -0.013109   -0.71616\n",
            "  0.55387002  1.14520001 -0.88044    -0.50216001 -0.22814     0.023885\n",
            "  0.1072      0.083739    0.55014998  0.58478999  0.75816     0.45706001\n",
            " -0.28001001  0.25224999  0.68965    -0.60971999  0.19577999  0.044209\n",
            " -0.31136    -0.68826002 -0.22721     0.46184999 -0.77161998  0.10208\n",
            "  0.55636001  0.067417   -0.57207     0.23735     0.47170001  0.82765001\n",
            " -0.29262999 -1.34220004 -0.099277    0.28139001  0.41604     0.10583\n",
            "  0.62203002  0.89495999 -0.23446     0.51349002  0.99378997  1.1846\n",
            " -0.16364001  0.20653     0.73853999  0.24059001 -0.96473002  0.13481\n",
            " -0.0072484   0.33015999 -0.12365     0.27191001 -0.40950999  0.021909\n",
            " -0.60689998  0.40755001  0.19566    -0.41802001  0.18636    -0.032652\n",
            " -0.78570998 -0.13846999  0.044007   -0.084423    0.04911     0.24104001\n",
            "  0.45273    -0.18682     0.46182001  0.089068   -0.18185    -0.01523\n",
            " -0.73680001 -0.14532     0.15104    -0.71493   ]\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(voc) \n",
        "# each word is represented by a vector of 100 floats (glove.6B.100d.txt)\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "missed_words = []\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "# word_index is a dictionary that maps each word to an index\n",
        "# we loop through all the words of word_index.items()\n",
        "# the items() method of a dictionary returns a list of tuples (key, index) for all elements in the dictionary\n",
        "for word, i in word_index.items():\n",
        "# we try to retrieve the vector of 100 floats for this word out of embeddings_index  \n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "# if we found the corresponding vector of 100 floats    \n",
        "    if embedding_vector is not None:\n",
        "      # we put the vector on position i of embedding_matrix\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1    \n",
        "    else:\n",
        "      # Words not found in embedding index will be all-zeros.    \n",
        "        misses += 1\n",
        "        missed_words.append(word)\n",
        "        \n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "print(\"*** shape of the embedding matrix:***\")\n",
        "print(embedding_matrix.shape)\n",
        "\n",
        "print(\"*** Missed words = words not in word_index ***\")\n",
        "print(missed_words[0:10])\n",
        "print()\n",
        "print(\"*** i has which index in word_index? ***\")\n",
        "index_i = word_index['i']\n",
        "print(index_i)\n",
        "print()\n",
        "print(\"*** the vector of 100 floats representing i ***\")\n",
        "print(embedding_matrix[index_i])\n",
        "print()\n",
        "print(\"*** cat has which index in word_index? ***\")\n",
        "index_cat = word_index['cat']\n",
        "print(index_cat)\n",
        "print()\n",
        "print(\"*** the vector of 100 floats representing cat ***\")\n",
        "print(embedding_matrix[index_cat])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aElU90qXVV0I"
      },
      "source": [
        "### Prepare the data\n",
        "First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays are right-padded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x07Drl07zwE",
        "outputId": "88ab9376-38ee-4e6f-c413-f920f3db4632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 111  905 3776    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [  14   53  365   83 1148 4837  174 5529  261    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [  53  273  192  106    6  203 1088  144    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [2376  125  144    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]\n",
            " [  14    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0]]\n",
            "(3900, 1)\n",
            "(1672, 1)\n",
            "(3900, 25)\n",
            "(1672, 25)\n",
            "----------------\n",
            "[1 1 1 ... 1 1 1]\n",
            "(3900,)\n",
            "(1672,)\n"
          ]
        }
      ],
      "source": [
        "X_train_final = vectorizer(np.array([s for s in X_train])).numpy()\n",
        "X_test_final = vectorizer(np.array([s for s in X_test])).numpy()\n",
        "\n",
        "y_train_final = np.array(y_train)\n",
        "y_test_final = np.array(y_test)\n",
        "\n",
        "print(X_train_final[:5])\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_train_final.shape)\n",
        "print(X_test_final.shape)\n",
        "print('----------------')\n",
        "print(y_train_final)\n",
        "print(y_train_final.shape)\n",
        "print(y_test_final.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build the model\n",
        "- Keras provides many preprocessing layers. We have seen earlier the `Flatten()` layer as an example.  \n",
        "- Another exampel is the `Embedding()` layer.  \n",
        "- It turns positive integers (indexes) into dense vectors of fixed size.  \n",
        "        e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
        "- We use it to load the pre-trained word embeddings matrix into an Embedding layer.\n",
        "- All that the Embedding layer does is to map the integer inputs to the vectors found at the corresponding index in the embedding matrix, so it will map for each sms message the index of each word to the coordinates in the geometry.\n",
        "- The `keras.layers.Embedding` layer handles an embedding matrix, which is trainable by default. \n",
        "- We need to set `trainable=False` so as to keep the embeddings fixed (we don't want to update them during training because we use pretrained word embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7474"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "\n",
        "def initial_model():\n",
        "    # we create a variable called model, and we set it equal to an instance of a Sequential object.\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Embedding(input_dim=num_tokens, output_dim=embedding_dim, input_length=25,\n",
        "                        embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False))\n",
        "    model.add(keras.layers.Flatten(input_shape=[num_tokens, embedding_dim]))\n",
        "\n",
        "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "    # Before we can train our model, we must compile it\n",
        "    # To the compile() function, we are passing the optimizer, the loss function, and the metrics that we would like to see. \n",
        "    # Notice that the optimizer we have specified is called Adam. Adam is just a variant of SGD. \n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer= tf.keras.optimizers.Adam(learning_rate = 0.01),\n",
        "                  metrics=['accuracy']) \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpGrdwWQ85iu",
        "outputId": "f889e35b-ab9f-417f-a753-17596650fcb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 25, 100)           747400    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2500)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               250100    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 202       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1028002 (3.92 MB)\n",
            "Trainable params: 280602 (1.07 MB)\n",
            "Non-trainable params: 747400 (2.85 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_1 = initial_model()\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:From c:\\Users\\jcor864\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "61/61 [==============================] - 5s 28ms/step - loss: 0.5959 - accuracy: 0.8713 - val_loss: 0.1726 - val_accuracy: 0.9330\n",
            "Epoch 2/20\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 0.1625 - accuracy: 0.9474 - val_loss: 0.1096 - val_accuracy: 0.9653\n",
            "Epoch 3/20\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.1073 - accuracy: 0.9674 - val_loss: 0.1548 - val_accuracy: 0.9438\n",
            "Epoch 4/20\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 0.0828 - accuracy: 0.9756 - val_loss: 0.1044 - val_accuracy: 0.9761\n",
            "Epoch 5/20\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 0.1035 - accuracy: 0.9723 - val_loss: 0.2217 - val_accuracy: 0.9492\n",
            "Epoch 6/20\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.1102 - accuracy: 0.9715 - val_loss: 0.1493 - val_accuracy: 0.9665\n"
          ]
        }
      ],
      "source": [
        "# We now add batch size to the mix of training parameters\n",
        "# If you don't specify batch size below, all training data will be used for each learning step\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=2,\n",
        "                                                  restore_best_weights=True)\n",
        "\n",
        "\n",
        "\n",
        "history_1 = model_1.fit(X_train_final, y_train_final,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_test_final, y_test_final),\n",
        "                    callbacks=[early_stopping_cb]\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "nCkT4JxmfP73",
        "outputId": "b1f680f1-62f7-46a8-d7da-2bde2fbb5c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set Accuracy:   0.99\n",
            "Training set Loss: 0.0245\n",
            "\n",
            "Validation set Accuracy:   0.98\n",
            "Validation set Loss: 0.1044\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# model_1 now contains the model at the end of the training run\n",
        "# We analyse the result:\n",
        "\n",
        "[train_loss, train_accuracy] = model_1.evaluate(X_train_final, y_train_final, verbose=0)\n",
        "print(\"Training set Accuracy:{:7.2f}\".format(train_accuracy))\n",
        "print(\"Training set Loss:{:7.4f}\\n\".format(train_loss))\n",
        "\n",
        "[val_loss, val_accuracy] = model_1.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "print(\"Validation set Accuracy:{:7.2f}\".format(val_accuracy))\n",
        "print(\"Validation set Loss:{:7.4f}\\n\".format(val_loss))\n",
        "\n",
        "#Now we visualise what happened during training\n",
        "plot_history(history_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fACX0ULsgDjG",
        "outputId": "0ffc207f-d3b6-46b1-ccad-ae005b65889d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 417ms/step\n",
            "[[9.9980754e-01 1.9247548e-04]]\n"
          ]
        }
      ],
      "source": [
        "X_example = vectorizer(np.array([s for s in [\"URGENT! Your Mobile No. was awarded a €2000 Bonus Caller Prize\"]])).numpy()\n",
        "pred = model_1.predict([X_example])\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbMA5Svgg6od",
        "outputId": "2c3693b6-eb1f-4042-a2e7-3b1aecb35c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 90ms/step\n",
            "[[4.3722692e-05 9.9995625e-01]]\n"
          ]
        }
      ],
      "source": [
        "X_example = vectorizer(np.array([s for s in [\"Please call me\"]])).numpy()\n",
        "pred = model_1.predict([X_example])\n",
        "print(pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Kopie van Using pretrained word embeddings.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
